{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Suppose we have 2 *sentences* :\n",
    "\n",
    "1. *Plant* need light and water.\n",
    "2. Everyone should *plant* more trees.\n",
    "\n",
    "Here the word **plant** has 2 different meanings in the above 2 sentences. In the first sentence, plant refers to a living organism and is used as a *Noun* whereas in the second sentence, plant has been used as a *Verb* which refer to put something in ground to grow.\n",
    "\n",
    "When we say these sentences to a person, the person would be able to differentiate between the meanings of the 2 sentences but suppose we want a machine to understand the meaning of the sentence, then we would  need to disambiguate the meaning of the word _plant_.\n",
    "\n",
    "Hence, if we are able to figure out the correct meaning of plant which is used in the sentence, we can infer the meaning of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to automate the process of finding the usage of the word based its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is POS Tagging ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tags are the **tags corresponding to the classification of the word as a Noun, Verb, Adjectives, etc**.\n",
    "\n",
    "In simple terms, POS(Parts-of-Speech) Tagging is nothing but to classify whether a word is a :-\n",
    "\n",
    "* Noun\n",
    "* Verb\n",
    "* Adjectives\n",
    "* Adverbs\n",
    "* Pronouns\n",
    "* Interjections\n",
    "* Conjunctions\n",
    "* Prepositions\n",
    "\n",
    "For example : The POS Tags for the words of the sentence : *I like playing chess* the POS Tags are as follows:\n",
    "\n",
    "* I       -> Pronoun\n",
    "* like    -> Verb\n",
    "* playing -> Verb\n",
    "* Chess   -> Noun\n",
    "\n",
    "It is possible that a single word has more than one POS Tag as it might appear in different context in different sentences, as seen for the above case(_plant_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS is useful because it :\n",
    "1. Gives a lot of information about a word and its neighbouring words\n",
    "2. Can be used for:\n",
    "    * Syntactic parsing of a sentence\n",
    "    * Named Entity Recognition \n",
    "    * Information Extraction\n",
    "    * Text to Speech conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do POS Tagging is tagging the words in the sentence manually.\n",
    "* Problems : \n",
    "   * Time consuming\n",
    "   * Not scalable\n",
    "* Solution :\n",
    "   * Try to ** automate the process** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automate the process of POS Tagging, we would use probabiltic approach to model our problem.\n",
    "\n",
    "We will consider *fine grained* tag sets. \n",
    "Fine grain tags sets are those sets in which tags are subdivided.For example: Tag Noun is broken into the following sub-parts:\n",
    "\n",
    "* Common\n",
    "* Proper\n",
    "* Abstract\n",
    "* Possessive\n",
    "* Collective\n",
    "\n",
    "Here each of the sub-tag will have a unique tag associated to it.\n",
    "\n",
    "Likewise, the other Tags (Verb, Adjective, ect) are also subdivided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model (HMM) POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model is trained on a fully annotated dataset in which each word in a sentence is annotated with a POS Tag.\n",
    "* We observe the words\n",
    "* The tags are hidden as they are not observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of sequence of words (W), find the corresponding sequence of tags for the given words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T<sup>opt</sup> = argmax<sub>T</sub> P(T|W)\n",
    "\n",
    "T<sup>opt</sup> = argmax<sub>T</sub>$\\frac{P(W|T)P(T)}{P(W)}$    (Using Bayes Theorem)\n",
    "\n",
    "T<sup>opt</sup> = argmax<sub>T</sub> P(W|T)P(T)    (since P(W) will remain same for all the tag sequence)\n",
    "\n",
    "Let W = w<sub>1</sub>w<sub>2</sub>...w<sub>n</sub>\n",
    "\n",
    "and T = t<sub>1</sub>t<sub>2</sub>...t<sub>n</sub>\n",
    "\n",
    "T<sup>opt</sup> = argmax<sub>T</sub> P(w<sub>1</sub>w<sub>2</sub>...w<sub>n</sub>|t<sub>1</sub>t<sub>2</sub>...t<sub>n</sub>)P(t<sub>1</sub>t<sub>2</sub>...t<sub>n</sub>)   \n",
    "\n",
    "T<sup>opt</sup> = argmax<sub>T</sub> $\\prod_{i=1}^{n}$ P(w<sub>i</sub>|t<sub>1</sub>t<sub>2</sub>...t<sub>n</sub>w<sub>1</sub>w<sub>2</sub>...w<sub>i-1</sub>)P(t<sub>i</sub>|t<sub>1</sub>t<sub>2</sub>...t<sub>i-1</sub>)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. P(w<sub>i</sub>|t<sub>1</sub>t<sub>2</sub>...t<sub>n</sub>w<sub>1</sub>w<sub>2</sub>...w<sub>i-1</sub>) = P(w<sub>i</sub>|t<sub>i</sub>) i.e. w<sub>i</sub> depends only on t<sub>i</sub> and not on other tags. We will call this **Emission Probability**\n",
    "\n",
    "\n",
    "2. P(t<sub>i</sub>|t<sub>1</sub>t<sub>2</sub>...t<sub>i-1</sub>) = P(t<sub>i</sub>|t<sub>i-1</sub>) i.e. t<sub>i</sub> depends only on t<sub>i-1</sub> and not on other tags. We will call this **Transmission Probability**\n",
    "\n",
    "\n",
    "3. P(t<sub>i</sub>|t<sub>i-1</sub>) = $\\frac{C(t_{i-1},t_{i})}{C(t_{i-1})}$  where \n",
    "    * C(t<sub>i-1</sub>,t<sub>i</sub>) denotes the number of times tag t<sub>i-1</sub> and t<sub>i</sub> appear together.\n",
    "    * C(t<sub>i-1</sub>) denotes the number of times tag t<sub>i-i</sub> appear in the corpus.\n",
    "    \n",
    "    \n",
    "    \n",
    "4. P(w<sub>i</sub>|t<sub>i</sub>) = $\\frac{C(w_{i},t_{i})}{C(t_{i})}$  where \n",
    "    * C(w<sub>i</sub>,t<sub>i</sub>) denotes the number of times word w<sub>i</sub> is assigned t<sub>i</sub> tag.\n",
    "    * C(t<sub>i</sub>) denotes the number of times tag t<sub>i</sub> appear in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above assumptions we get : \n",
    "T<sup>opt</sup> = argmax<sub>T</sub> $\\prod_{i=1}^{n}$ P(w<sub>i</sub>|t<sub>i</sub>)P(t<sub>i</sub>|t<sub>i-1</sub>) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following image\n",
    "![viterbi](viterbi.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, for each tag of a word we will store which previous tag gives the highest probability.\n",
    "\n",
    "Consider two 2-D Matrices\n",
    "\n",
    "    1. Viterbi\n",
    "    2. Index\n",
    "    \n",
    "Viterbi[i][j] = maximum probability of the sequence of words w<sub>1</sub>w<sub>2</sub>...w<sub>j</sub> if the tag of w<sub>j</sub> is the i<sup>th</sup> tag corresponding to w<sub>j</sub>\n",
    "\n",
    "Index[i][j] will store the index of the tag of the previous word which corresponds to the maximum probabilty for the j<sup>th</sup> word to have i<sup>th</sup> tag\n",
    "\n",
    "The size of both the matrix will be T $\\times$N where\n",
    "   * T = Maximum tags of any word in the sentence.\n",
    "   * N = Number of words in the sentence.\n",
    "\n",
    "   \n",
    "In the above example the size of the matrix is 4 $\\times$5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Cells of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing cells (i,j) for the matrix, use the following relation:\n",
    "\n",
    "Viterbi[i][j]=$ max_{k=1..T}${ Viterbi[k][j-1] $\\times$ P(i<sup>th</sup> tag corresponding to the j<sup>th</sup> word | k<sup>th</sup> tag corresponding to the (j-1)<sup>th</sup> word) $\\times$ P(j<sup>th</sup> word | i<sup>th</sup> tag corresponding to the j<sup>th</sup> word)} \n",
    "\n",
    "Index[i][j]=$ argmax_{k=1..T}${ Viterbi[k][j-1] $\\times$ P(i<sup>th</sup> tag corresponding to the j<sup>th</sup> word | k<sup>th</sup> tag corresponding to the (j-1)<sup>th</sup> word) $\\times$ P(j<sup>th</sup> word | i<sup>th</sup> tag corresponding to the j<sup>th</sup> word)} \n",
    "\n",
    "\n",
    "For the above example suppose we want to compute Viterbi[2][4], then \n",
    "\n",
    "Viterbi[2][4]=$ max_{k=1..4}${ Viterbi[k][3] $\\times$ P(DT|k<sup>th</sup> tag corresponding to the word *back*(if k=1,then it is RB)) $\\times$ P(the|DT)} \n",
    "\n",
    "\n",
    "The initialisations of the matrix can be done as follows:\n",
    "\n",
    "Let S be the POS tag for start symbol, then \n",
    "\n",
    "dp[i][1] = =$ max_{k=1..T}${P((i<sup>th</sup> tag corresponding to the 1<sup>st</sup> word | S) $\\times$ P(1<sup>st</sup> word | i<sup>th</sup> tag corresponding to the 1<sup>st</sup> word)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking the optimal sequence of tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Backtrack the optimal sequence of tags for the given word sequence we will use the Index matrix.\n",
    "\n",
    "1. Find $ argmax_{k=1..T}$Viterbi[k][N]\n",
    "\n",
    "2. Iteratively, trace back to the appropriate index using the Index matrix until we have reached the 1<sup>st</sup> word. \n",
    "\n",
    "3. Also, at each iteration store the corresponding tag of the word.\n",
    "\n",
    "The sequence of tags obtained willbe the optimal sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time = O(NT<sup>2</sup>)\n",
    "2. Space = O(NT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implmenting of the above mentioned algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I am importing all the libraries that will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the words and tags from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used BERP dataset for training our model . i.e. computing emmission and transmission probabilities.\n",
    "* Extracting the words along with their tags. \n",
    "* Next, I have separated the words and tags.\n",
    "* Stored the different tags which have appeared for a word\n",
    "* Maintaing the unigram count of the word and the tag appearing in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Train_Viterbi.txt','r')\n",
    "\n",
    "words={}\n",
    "tags_unigram={}\n",
    "sentences=f.readlines()\n",
    "possible_tags={}\n",
    "tags_count=0\n",
    "words_count=0\n",
    "for i in range(len(sentences)):\n",
    "    word_tag=sentences[i].split('\\t')\n",
    "    if(len(word_tag)==2):\n",
    "        word=word_tag[0]\n",
    "        tag=word_tag[1]\n",
    "        tag=tag[:len(tag)-1]\n",
    "        if(word=='.' or word=='?' or word=='!'):\n",
    "            word='<s>'\n",
    "            tag='<s>'\n",
    "            sentences[i]='<s>'+'\\t'+'<s>'\n",
    "        if(word not in possible_tags):\n",
    "            possible_tags[word]=[tag]\n",
    "        else:\n",
    "            if(tag not in possible_tags[word]):\n",
    "                possible_tags[word].extend([tag])\n",
    "        words[word]=words.get(word,0)+1\n",
    "        tags_unigram[tag]=tags_unigram.get(tag,0)+1\n",
    "        tags_count+=1\n",
    "        words_count+=1\n",
    "    if(sentences[i]=='\\n'):\n",
    "        sentences[i]='<s>'+'\\t'+'<s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We would be calculating the Emmission and Transmission Probabilities from our training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Transmission Probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the count of all possible tag bigrams which apperar in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_bigram={}\n",
    "for i in range(len(sentences)-1):\n",
    "    tag_1=sentences[i].split('\\t')[1]\n",
    "    tag_2=sentences[i+1].split('\\t')[1]\n",
    "    if(tag_1!='<s>'):\n",
    "        tag_1=tag_1[:len(tag_1)-1]\n",
    "    if(tag_2!='<s>'):\n",
    "        tag_2=tag_2[:len(tag_2)-1]\n",
    "    tag=tag_1+\" \"+tag_2\n",
    "    tags_bigram[tag]=tags_bigram.get(tag,0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding the transmission probabilites using the bigram count and the unigram count of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_prob={}\n",
    "\n",
    "for i in tags_bigram:\n",
    "    split=i.split()\n",
    "    key=split[0]\n",
    "    transmission_prob[i]=tags_bigram[i]/(1.0*tags_unigram[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Emmision Probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the tag-word frequency(count) - i.e. the number of times a word and a tag appeared together in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_word={}\n",
    "for i in range(len(sentences)):\n",
    "    line =sentences[i].split('\\t')\n",
    "    word=line[0]\n",
    "    tag=line[1]\n",
    "    if(tag!='<s>'):\n",
    "        tag=tag[:len(tag)-1]\n",
    "    key=tag+\" \"+word\n",
    "    tag_word[key]=tag_word.get(key,0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the Emmision probabilities using the tag-word frequncy and the unigram count of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_prob={}\n",
    "for i in tag_word:\n",
    "    split=i.split()\n",
    "    tag=split[0]\n",
    "    emission_prob[i]=tag_word[i]/(1.0*tags_unigram[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We take an input sentence to test our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"i like food\"\n",
    "\n",
    "tokens=[]\n",
    "tokens.append('<s>')\n",
    "tokens.extend(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will run the Viterbi algorithm\n",
    "* To do this,we would need the following things\n",
    "    * 2 matrix for\n",
    "        1. Storing the calculated probabilities in each step of the iteration of the algorithm (The required matrix is viterbi_prob).\n",
    "        2. Storing the tag of the previous word ((i-1)<sup>th</sup>) which is most probable to for the j<sup>th</sup> tag of the i<sup>th</sup> word.(The required matrix is viterbi_prob_index)\n",
    "    * Transmission probabilties\n",
    "    * Emission probabilties\n",
    "* I am printing the viter_prob matrix at each iteration so as to better understand the working of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word 1 Tag 1\n",
      "\n",
      "Matrix : \n",
      "[[0.         0.21458124 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "\n",
      "Word 2 Tag 1\n",
      "\n",
      "Matrix : \n",
      "[[0.         0.21458124 0.00591492 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Word 2 Tag 2\n",
      "\n",
      "Matrix : \n",
      "[[0.         0.21458124 0.00591492 0.        ]\n",
      " [0.         0.         0.00235987 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Word 2 Tag 3\n",
      "\n",
      "Matrix : \n",
      "[[0.00000000e+00 2.14581243e-01 5.91492077e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.35986886e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.03415876e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Word 2 Tag 4\n",
      "\n",
      "Matrix : \n",
      "[[0.00000000e+00 2.14581243e-01 5.91492077e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.35986886e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.03415876e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.50111984e-05 0.00000000e+00]]\n",
      "\n",
      "\n",
      "Word 3 Tag 1\n",
      "\n",
      "Matrix : \n",
      "[[0.00000000e+00 2.14581243e-01 5.91492077e-03 4.72852442e-05]\n",
      " [0.00000000e+00 0.00000000e+00 2.35986886e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.03415876e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.50111984e-05 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "viterbi_prob=np.zeros((len(tokens),len(tokens)))\n",
    "viterbi_prob_index=np.zeros((len(tokens),len(tokens)))\n",
    "w=1\n",
    "\n",
    "for i in range(1,len(tokens)):\n",
    "    print\n",
    "    possibleCurrentTags=possible_tags[tokens[i]]\n",
    "    previousTags=possible_tags[tokens[i-1]]\n",
    "    current_word=tokens[i]\n",
    "    t=1\n",
    "    for j in range(len(possibleCurrentTags)):\n",
    "        print\n",
    "        print \"Word \"+str(w)+\" Tag \"+str(t)\n",
    "        print\n",
    "        t+=1\n",
    "        if(i==1):\n",
    "            current_tag=possibleCurrentTags[j]\n",
    "            maximum=-1\n",
    "            index=-1\n",
    "            for k in range(len(previousTags)):\n",
    "                previous_tag=previousTags[k]\n",
    "                possible_ans=transmission_prob[previous_tag+\" \"+current_tag]*emission_prob[current_tag+\" \"+current_word]\n",
    "                if(maximum<possible_ans):\n",
    "                    maximum=possible_ans\n",
    "                    index=k\n",
    "            viterbi_prob[j][i]=maximum\n",
    "            viterbi_prob_index[j][i]=index\n",
    "        else:\n",
    "            current_tag=possibleCurrentTags[j]\n",
    "            maximum=-1\n",
    "            index=-1\n",
    "            for k in range(len(previousTags)):\n",
    "                previous_tag=previousTags[k]\n",
    "                possible_ans=viterbi_prob[k][i-1]*transmission_prob[previous_tag+\" \"+current_tag]*emission_prob[current_tag+\" \"+current_word]\n",
    "                if(maximum<possible_ans):\n",
    "                    maximum=possible_ans\n",
    "                    index=k\n",
    "            viterbi_prob[j][i]=maximum\n",
    "            viterbi_prob_index[j][i]=index\n",
    "        print \"Matrix : \\n\" ,viterbi_prob\n",
    "    w+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the correct combination of the tags of the word which gives the maximum probability of the given sentence.\n",
    "* We will use viterbi_prob_index matrix for finding the correct tag of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans={}\n",
    "i=len(tokens)-1\n",
    "maximum=-1\n",
    "index=-1\n",
    "for j in range(len(tokens)):\n",
    "    if(viterbi_prob[j][i]>maximum):\n",
    "        maximum=viterbi_prob[j][i]\n",
    "        index=j\n",
    "while(i>0):\n",
    "    ans[tokens[i]]=possible_tags[tokens[i]][int(index)]\n",
    "    index=viterbi_prob_index[int(index)][i]\n",
    "    i-=1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the final output of the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i --> PRP\n",
      "like --> VB\n",
      "food --> NN\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(tokens)):\n",
    "    print tokens[i] + \" --> \"+ ans[tokens[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Jurafsky, Dan. Speech & language processing. Pearson Education India, 2000.\n",
    "2. https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24\n",
    "3. Class Notes\n",
    "\n",
    "**Image Reference**\n",
    "\n",
    "1. Jurafsky, Dan. Speech & language processing. Pearson Education India, 2000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
